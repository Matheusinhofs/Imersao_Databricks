- Imersão Databricks

---- Aula 01

    - O que é um data lakehouse?
        - O lakehouse armazena dados que pode ser estruturados e não estruturados, mas com uma camada de estruturação 
    - O que é um data lake?
        - É um local em que armazenamos dados estruturados e não estruturados, se mantendo todos em um mesmo lugar
    
    - Teremos nossa fonte de dados (Sistema de Vendas, API Bitcoin, API Yahoo)
        - Para extrair podemos usar: Pentaho, SSIS, NiFi, Airbyte, Fivetrain, Script Python, Datafactory,... 
    - Em seguida, o dado é armazenado no data lake, que normalmente é bem barato.
    - Por último ele é salvo no lakehouse, com a estrutura medalhão (Bronze, Silver, Gold)
        - A camada bronze deve ser a mesma que o sistema de transação, de forma que é possível realizar as querys na camada bronze
    - O databricks ele se posiciona como uma plataforma em que você pode realizar tudo do seu processo de dados
    - No catalog ele gerencia os dados
    - No Job é onde orquestra e automatiza tarefas
    - Compute é como irá rodar o spark
    - Vários tipos de conectores (SAP, API, Databases,...)
    - Notebook do Databriks roda alguns tipos de linguagens, incluindo Python
    - Várias bibliotecas estão já instaladas
    - Em seguida é criado o catálogo para o lakehouse. E dentro dele os schemas do modelo medalhão.
    - O catálogo é a maior hierarquia dentro do databricks
    - No databricks é possível criar acessos para as áreas de negócio de cada schema (Settings -> Identity and access -> Groups)
    - O volume é um arquivo de protocolo de S3 (um storage) 
    - Criamos os jobs que vão agendar quando o script vai rodar 
    - Ele permite que seja escalável
    - O storage é desvinculado da engine do databricks
    - No databricks podemos criar jobs e pipelines, com o lakeflow por exemplo
        - Lakeflow permite rodar tanto em python como em sql
        - Com o SQL você pode indicar parâmetros para mudanças no schema, ou seja, toda vez que a tabela no sistema mudar, o que ele vai fazer
    - O catalog do databricks permite que você possa comentar na tabela, identificar o tipo dela, ter uma amostra dos dados e etc.
    - Muito útil para entendimento dos campos e valores da sua tabela
    
---- Aula 02    
    
    - Requisitos de projetos
        - Buscar as tabelas utilizadas e entender os campos dela 
        - Saber quais as funções das tabelas
    - Projeto de dados é iniciado pela área de negócio, entendendo o que precisa e etc.
    - Em seguida é necessário buscar as fontes, entender as tabelas e desenvolver seu projeto  
    - Mapa de oportunidade - Velocidade vs confiança (e tem valor para cada coisa) 
    - Ter um requisito de projeto longo para poder entregar um produto de dados 
    - Além disso é importante definir quais entregas entram em cada parte da matriz de velocidade vs confiança
    - Os que são lentos e tem pouca confiança podem gerar conflitos com o cliente/área
    - Os que tem mais velocidade e confiança é positivo por gerar uma entrega mais direta
    - Um bom requisito do projeto possui:
        - Cabeçalho: com área de negócio, datas, status atual
        - Contexto e impacto: definição do problema e desafios atuais, objetivo, impacto esperado, indicadores alvos
        - Pontos de atenção: coisas que podem limitar o desenvolvimento do projeto
    - Em seguida é definida a arquitetura medalhão
    - Depois é feita a modelagem dos dados, definindo as tabelas que devem estar presentes em cada camada 

    - Desafio Erathos 

----- Aula 03
    - 




















